{"cells":[{"cell_type":"code","metadata":{"tags":[],"cell_id":"00000-2fa6723e-68a3-4d2c-8c3d-d5e583622760","deepnote_cell_type":"sql"},"source":"","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These are the main methods for the **LSH algorithm**. One sets up the main data structure, the LSH Cache. And the other computes candidate duplicates.","metadata":{"tags":[],"cell_id":"00000-2e5b3d61-114a-488c-b659-c5f1e46d9f1a","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00000-527ea9b8-9028-48e7-a757-659f7f96867f","output_cleared":false,"source_hash":"a5f3f168","execution_millis":5,"execution_start":1608225773580,"deepnote_cell_type":"code"},"source":"import itertools\nimport json\nfrom lsh import cache, minhash # https://github.com/mattilyra/lsh\n\n# Each bin = band = use of a hash function. bins contain buckets. \n# Each bucket = group of similar text according to bin's hash fn\ndef candidate_duplicates(lshcache):\n    candidate_pairs = set()\n    for b in lshcache.bins:\n        for bucket_id in b:\n            if len(b[bucket_id]) > 1:\n                pairs_ = set(itertools.combinations(b[bucket_id], r=2))\n                candidate_pairs.update(pairs_)\n    return candidate_pairs\n\ndef create_cache(sstubs, char_ngram=5, seeds=100, bands=5, hashbytes=4):\n    hasher = minhash.MinHasher(seeds=seeds, char_ngram=char_ngram, hashbytes=hashbytes)\n    if seeds % bands != 0:\n        raise ValueError('Seeds has to be a multiple of bands. {} % {} != 0'.format(seeds, bands))\n    \n    lshcache = cache.Cache(num_bands=bands, hasher=hasher)\n    for sstub_id, sstub in enumerate(sstubs):\n        code = sstub[\"sourceBeforeFix\"]\n        lshcache.add_fingerprint(hasher.fingerprint(code), doc_id=sstub_id)\n    return lshcache\n    \n","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is script runs the LSH algorithm on our SStuBs dataset. It also produces a list of candidate duplicates.","metadata":{"tags":[],"cell_id":"00002-443d10dc-d539-4813-bbab-fa8f4d66c43f","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00001-1f1f97b9-5350-4d8c-ae79-1d256e0296a6","output_cleared":false,"source_hash":"461321ed","execution_millis":4053,"execution_start":1608225776971,"deepnote_cell_type":"code"},"source":"with open(\"modified-dataset.json\", 'r') as f:\n    sstubs = json.load(f)\n    print(\"Total number of SStuBs: \", len(sstubs))\n\nlshcache = create_cache(sstubs, char_ngram=5, seeds=100, bands=1, hashbytes=4)\ncandidate_pairs = candidate_duplicates(lshcache)\n","execution_count":null,"outputs":[{"name":"stdout","text":"Total number of SStuBs:  29057\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The final script groups SStuBs into buckets. It adds attribute \"bucketHash\" to each SStuB elements and writes the new dataset into file \"sstubs-0104-bucket-hash.json\"","metadata":{"tags":[],"cell_id":"00004-6b3e62f6-d965-48eb-bad3-0c5aff664b4e","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00004-1e17fa68-0524-4d32-a61a-03fadb51865d","output_cleared":false,"source_hash":"dae28ffd","execution_millis":1324,"execution_start":1608225830109,"deepnote_cell_type":"code"},"source":"# We will only look at clone groups in the first bin. \n# Threat to validity: False negative rate might be high since we did not go through all bins\nfirst_bin = lshcache.bins[0]\n\nprint(\"Number of buckets: \", len(first_bin))\n\nfor bucket_id in first_bin:\n    for sstub_index in first_bin[bucket_id]:\n        sstubs[sstub_index][\"bucketHash\"] = bucket_id\n\nwith open(\"sstubs-0104-bucket-hash.json\", \"w\") as f:\n    json.dump(sstubs, f, sort_keys=True)\n\n\nwith open(\"sstubs-0104-grouped.json\", \"w\") as f:\n    sstub_map = {}\n    for bucket_id in first_bin:\n        bucket = []\n        for sstub_index in first_bin[bucket_id]:\n            bucket.append(sstubs[sstub_index])\n        sstub_map[bucket_id] = bucket\n\n    json.dump(sstub_map, f, sort_keys=True)","execution_count":null,"outputs":[{"name":"stdout","text":"Number of buckets:  13439\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Random code snippet for analysing random stuff. Currently total # of SStuBs and projects.","metadata":{"tags":[],"cell_id":"00007-1421455b-e2bf-4bdb-adee-1a4286fd38ad","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00007-72d100b6-b4a9-4658-a676-799ecf00883a","output_cleared":false,"source_hash":"413e6a58","execution_millis":603,"execution_start":1608226188791,"deepnote_cell_type":"code"},"source":"import json\nwith open(\"modified-dataset.json\", \"r\") as f:\n    s = set()\n    sstubs = json.load(f)\n    print(\"SStuBs in total: \", len(sstubs))\n    for stub in sstubs:\n        s.add(stub[\"projectName\"])\n    print(\"Number of projects in total: \", len(s))","execution_count":null,"outputs":[{"name":"stdout","text":"SStuBs in total:  29057\nNumber of projects in total:  410\n","output_type":"stream"}]}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"70840540-f4a5-4198-852e-d8bf748a8137","deepnote_execution_queue":[]}}